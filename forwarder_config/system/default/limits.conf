#   Version 6.4.0
# DO NOT EDIT THIS FILE!
# Changes to default files will be lost on update and are difficult to
# manage and support.
#
# Please make any changes to system defaults by overriding them in
# apps or $SPLUNK_HOME/etc/system/local
# (See "Configuration file precedence" in the web documentation).
#
# To override a specific setting, copy the name of the stanza and
# setting to the file where you wish to override it.
#
# This file configures various limits to the Splunk's search commands.
# CAUTION: Do not alter the settings in limits.conf unless you know what you are doing.
#
# Improperly configured limits may result in splunkd crashes and/or memory overuse.


[searchresults]
maxresultrows = 50000
# maximum number of times to try in the atomic write operation (1 = no retries)
tocsv_maxretry = 5
# retry period is 1/2 second (500 milliseconds)
tocsv_retryperiod_ms = 500

compression_level = 1

[search_info]
# These setting control logging of error messages to info.csv
# All messages will be logged to search.log regardless of these settings.
# maximum number of error messages to log in info.csv
# Set to 0 to remove limit, may affect search performance
max_infocsv_messages  = 20
# log level = DEBUG | INFO | WARN | ERROR
infocsv_log_level = INFO
#Log warnings if search returns no results because user has no permissions to search on queried indexes
show_warn_on_filtered_indexes = false
#log level of messages when search results no results because user has no permissions to search on queries indexes
filteredindexes_log_level = DEBUG

[subsearch]
# maximum number of results to return from a subsearch
maxout = 10000
# maximum number of seconds to run a subsearch before finalizing
maxtime = 60
# time to cache a given subsearch's results
ttl = 300

[lookup]
# maximum size of static lookup file to use a in-memory index for
max_memtable_bytes = 10000000
# maximum matches for a lookup
max_matches = 1000
# maximum reverse lookup matches (for search expansion)
max_reverse_matches = 50
# default setting for if non-memory file lookups (for large files) should batch queries
# can be override via a lookup table's stanza in transforms.conf
batch_index_query = true
# when doing batch request, what's the most matches to retrieve
# if more than this limit of matches would otherwise be retrieve, we will fall back to non-batch mode matching
batch_response_limit = 5000000
# maximum number of lookup error messages that should be logged
max_lookup_messages = 20


[anomalousvalue]
maxresultrows = 50000
# maximum number of distinct values for a field
maxvalues = 0
# maximum size in bytes of any single value (truncated to this size if larger)
maxvaluesize = 0

[associate]
maxfields = 10000
maxvalues = 0
maxvaluesize = 0

[autoregress]
maxp = 10000
maxrange = 1000

[concurrency]
# maximum concurrency level to keep record of
max_count = 10000000

# for the contingency, ctable, and counttable commands
[ctable]
maxvalues = 1000

[correlate]
maxfields = 1000

# for bin/bucket/discretize
[discretize]
maxbins = 50000
# if maxbins not specified or = 0, defaults to searchresults::maxresultrows

[inputcsv]
# maximum number of retries for creating a tmp directory (with random name in SPLUNK_HOME/var/run/splunk)
mkdir_max_retries = 100

[indexpreview]
# maximum number of bytes to read from each file during preview
max_preview_bytes = 2000000
# maximum number of results to emit per call to preview data generator
max_results_perchunk = 2500
# loosely-applied maximum on number of preview data objects held in memory
soft_preview_queue_size = 100


[inputproc]
# Threshold size (in mb) to trigger fishbucket rolling to a new db
file_tracking_db_threshold_mb = 500
# Approximate ceiling on sourcetypes & fingerprints in learned app
learned_sourcetypes_limit = 1000

# Maximum size (in mb) of heap allowed to be created by Splunk modular input MonitorNoHandle. 
monitornohandle_max_heap_mb = 0

[join]
subsearch_maxout = 50000
subsearch_maxtime = 60
subsearch_timeout = 120

[kmeans]
maxdatapoints = 100000000
maxkvalue = 1000
maxkrange = 100

[typeahead]
maxcount          = 1000
fetch_multiplier  = 50
use_cache         = true
cache_ttl_sec     = 300
min_prefix_length = 1
max_concurrent_per_user = 3

[kv]
# when non-zero, the point at which kv should stop creating new columns
maxcols  = 512
# maximum number of keys auto kv can generate
limit    = 100
# truncate _raw to to this size and then do auto KV
maxchars = 10240

max_extractor_time = 1000
avg_extractor_time = 500

[metadata]
maxresultrows = 10000
# the most metadata results to fetch from each indexer.
maxcount = 100000

[metrics]
# the number of series to include in the per_x_thruput reports in metrics.log
maxseries = 10

[metrics:tcpin_connections]
# keep each connection metrics
aggregate_metrics = false
# keep _tcp_Bps, _tcp_KBps, _tcp_avg_thruput, _tcp_Kprocessed that can be derived from kb
suppress_derived_info = false

[rare]
maxresultrows = 50000
# maximum distinct value vectors to keep track of
maxvalues = 0
maxvaluesize = 0

[restapi]
# maximum result rows to be return by /events or /results getters from REST API
maxresultrows = 50000

# regex constraint on time_format and output_time_format for search endpoints
time_format_reject = [<>!]

# truncate the properties over this length in the contents dictionary of a job entry from the jobs endpoint
# 0 means don't truncate
jobscontentmaxcount = 0

[search_metrics]
# Add more detail to the per-search metrics
debug_metrics = false

[search]
search_process_mode = auto

# On UNIX we can run more that one search per process; set this to a number greater than one to enable
max_searches_per_process = 1

# When running more than one search per process, don't allow a process to
# accumulate more than this number of seconds running searches.  Note that a
# search can run longer than this without being terminated, it only prevents
# the process from being used for another search
max_time_per_process = 300.0

# When running more than one search per process, don't reuse a process if it is older
# than this number of seconds.  This is different than max_time_per_process
# because it includes time the process spent idle
process_max_age = 7200.0

# Periodically we'll check if we have too many idle search processes; this controls how often that happens (in seconds)
idle_process_reaper_period = 30.0

# Don't reuse a process that last served a different user unless its been idle this long (in seconds)
process_min_age_before_user_change = 4.0

# Number of server threads dedicated to managing communication with search processes
# Negative number means automatically pick a sensible value
launcher_threads = -1

# When running a search, scan at most this many idle processes before launching a new one
launcher_max_idle_checks = 5

# When reaping idle search processes, allow one to be reaped if it is not using
# the most recent configuration bundle, and its bundle hasn't been used in at
# least this many seconds
max_old_bundle_idle_time = 5.0

# If a search process is idle for this many seconds, take the opportunity
# to scan its internal caches for stale data
idle_process_cache_timeout = 0.5

# Even if the search process has not been idle for the above time, check its
# internal caches for stale data after this many searches
idle_process_cache_search_count = 8

# Inside a search process, keep up to this many compiled regex artifacts before
# checking for stale ones.  Normally the above idle_process_cache_* settings
# will check for stale entries before this limit is hit
idle_process_regex_cache_hiwater = 2500

# Corresponds to the size of the results queue in the dispatch fetch level
result_queue_max_size = 100000000
# Defaults to download all remote logs other than saved search logs and oneshot search logs
fetch_remote_search_log = disabledSavedSearches

# use precomputed summaries if possible?
summary_mode = all

# by default use bloom filter
use_bloomfilter = true

# max length of custom job id when passing spawning new job
max_id_length = 150

# how long searches should be stored on disk once completed
ttl = 600

# how long searches should be stored on disk once failed
failed_job_ttl = 86400

# how long should searches run for a search head live on the indexers
remote_ttl = 600

# by default, no timeline information is retained.  UI will supply the status_buckets as needed
status_buckets = 0

# the maximum number of end results to store globally (when status_buckets=0)
max_count = 500000

# truncate report output to max_count?
truncate_report = false

# the minimum length of a prefix before a * to ask the index about
min_prefix_len = 1

# the length of time to persist search cache entries (in seconds)
cache_ttl = 300

# maximum results per call to search (in dispatch), must be <= maxresultrows
max_results_perchunk = 2500

# minimum reuslts per call to search (in dispatch), must be <= max_results_perchunk
min_results_perchunk = 100

# maximum raw size of results per call to search (in dispatch), 0 = no limit, not affected by chunk_multiplier
max_rawsize_perchunk = 100000000

# target duration of a particular call to fetch search results in ms
target_time_perchunk = 2000

# time in seconds until a search is considered "long running"
long_search_threshold = 2

# max_results_perchunk, min_results_perchunk, and target_time_perchunk
# are multiplied by this for a long running search
chunk_multiplier = 5

# the minimum frequency of a field displayed in the /summary endpoint
min_freq = 0.01

# the frequency with which try to reduce intermediate data when there is an non-streaming and non-stateful streaming command. (0 = never)
reduce_freq = 10

# the maximum time to spend doing reduce, as a fraction of total search time
reduce_duty_cycle = 0.25

# the maximum time to spend generating previews, as a fraction of total search time
preview_duty_cycle = 0.25

# the minimum number of results blobs to keep for consumption by the search head.
results_queue_min_size = 10

# the maximum number of times to retry to dispatch a search when the quota has been reached
dispatch_quota_retry = 4

# milliseconds between retrying to dispatch a search if a quota has been reached
# we retry the given number of times, with each successive wait 2x longer than the previous
dispatch_quota_sleep_ms = 100

# the maximum number of concurrent searches per CPU
max_searches_per_cpu = 1

# the base number of concurrent searches
base_max_searches = 6

# max real-time searches = max_rt_search_multiplier x max historical searches
max_rt_search_multiplier = 1

# the total number of concurrent searches is base_max_searches + #cpus*max_searches_per_cpu

# max recursion depth for macros
# considered a search exception if macro expansion doesn't stop after this many levels
max_macro_depth = 100

# max recursion depth for subsearch
# considered a search exception if subsearch doesn't stop after this many levels
max_subsearch_depth = 8

# for real-time searches in the UI, maximum number of events stored (as a FIFO buffer)
realtime_buffer = 10000

# stack size of the search executing thread
stack_size = 4194304

# the number of search job metadata to cache in RAM
status_cache_size = 10000

# search results combiner maximum in-memory buffer size (in events)
max_combiner_memevents = 50000

# the minimum bundle replication period
replication_period_sec  = 60

# bundle replication file ttl
replication_file_ttl = 600

# whether bundle replication is synchronous (and thus blocking searches)
sync_bundle_replication = auto

# when doing round-robin fetching, what are the min,max,and backoff factors for the amount
# of time (in ms) to sleep when no data is available
rr_min_sleep_ms = 10
rr_max_sleep_ms = 1000
rr_sleep_factor = 2

# how often to update the field summary statistics, as a ratio to the elapsed run time so far
fieldstats_update_freq = 0
# maximum period for updating field summary statistics in seconds
fieldstats_update_maxperiod = 60

# allow timeline to be map/reduced?
remote_timeline = true
# minimum peers required to utlize remote timelining
remote_timeline_min_peers = 1
# how often to touch remote artifacts to keep them from being reaped when search has not finished? (in seconds)
remote_timeline_touchperiod = 300
# whether to fetch all events accessible through the timeline from the remote peers before the job is considered done
remote_timeline_fetchall = 1

# timeouts for fetching remote timeline events
remote_timeline_connection_timeout = 5
remote_timeline_send_timeout = 10
remote_timeline_receive_timeout = 10

# size of thread pool for remote event download framework
remote_event_download_initialize_pool = 5
remote_event_download_finalize_pool = 5
remote_event_download_local_pool = 5

# default setting for allowing async jobs to be queued if quota violation
default_allow_queue = true

# how often to retry queued jobs (in seconds)
queued_job_check_freq = 1

# enable search history?
enable_history = true

# max number of searches to store in history (per user/app)
max_history_length = 1000

# allow inexact metasearch?
allow_inexact_metasearch = false

dispatch_dir_warning_size = 5000

# track indextime range of searches (shown in job inspector)
track_indextime_range = true

# avoid loading remote bundles in splunkd
load_remote_bundles = false

# allow batch mode which searches in non-time order for certain classes of searches
allow_batch_mode = true

# when in batch mode what is the max number of index values to read in at one time
batch_search_max_index_values = 10000000

# When batch mode attempts to retry the search on a peer that failed wait at least this many seconds
batch_retry_min_interval = 5

# When batch mode attempts to retry the search on a peer that failed wait at most this many seconds
batch_retry_max_interval = 300

# After a retry attempt fails increase the time to wait before trying again by this scaling factor
batch_retry_scaling = 1.5

# If all currently active peers have finished with the search wait this many seconds before giving up on
# peers we are attempting to reconnect to for a retry.
batch_wait_after_end = 900

#Number of search pipelines created per batch search
batch_search_max_pipeline = 1

#Default size of the aggregator queue to which all the search pipelines dump the search results on the indexer.
batch_search_max_results_aggregator_queue_size = 100000000

#Default size of the serialized results queue where all the serialized results are kept before transmission.
batch_search_max_serialized_results_queue_size = 100000000

# how long jobs are saved for by default
default_save_ttl = 604800

#do we write multi file results to results_dir
write_multifile_results_out = true

#maximum number of worker threads in Round Robin policy
max_workers_searchparser = 5

#maximum size of the chunk queue
max_chunk_queue_size = 1000000

#absolute value of largest timeskew we will tolerate between the searchead and the peer (in seconds)
max_tolerable_skew = 60
# Limit on the skew permitted when adding a search peer (peers with a skew larger than this will be rejected)
addpeer_skew_limit = 600

#Default value for memory usage for the splunk search process is set to 4GB.
search_process_memory_usage_threshold = 4000

#Default value for percentage memory usage for the splunk search process is set to 25%.
search_process_memory_usage_percentage_threshold = 25

# Enable concatenation of successively occuring evals into a single
# comma seperated eval during generation of datamodel searches.
enable_datamodel_meval = true

# If memory tracker is disabled, search won't be terminated even if it exceeds the memory limit
# By default disable memory tracking
enable_memory_tracker = false

# Enable expanded search string pruning
enable_expanded_search_pruning = false

[realtime]
# default options for indexer support of real-time searches
# these can all be overriden for a single search via REST API arguments

# size of queue for each real-time search
queue_size = 10000

# should indexer block if a queue is full?
blocking = false

# maximum time to block if the queue is full (meaningless if blocking = false)
max_blocking_secs = 60

# should the indexer prefilter events for efficiency?
indexfilter = true

# should real-time windowed searches backfill with historical data by default?
default_backfill = true

# should real-time windowed searches sort events to be in descending time order?
enforce_time_order = true

# should we use indexedRealtime by default
indexed_realtime_use_by_default = false

# number of seconds to wait for disk flushes to finish with indexed/continuous/psuedo realtime search
indexed_realtime_disk_sync_delay = 60

# minimum seconds to wait between component index searches during an indexed realtime search
indexed_realtime_default_span = 1

# max number of seconds allowed to fall behind realtime before we drop data
# and reset back to the default span from realtime.
indexed_realtime_maximum_span = 0

# frequency to fetch updated bucket list from daemon
indexed_realtime_cluster_update_interval = 30

# this limits the frequency that we will trigger alerts during a realtime search
alerting_period_ms = 0

[set]
maxresultrows = 50000


[slc]
# maximum number of clusters to create
maxclusters = 10000

[findkeywords]
maxevents = 50000

[sort]
# maximum number of concurrent files to open
maxfiles = 64

[spath]
# number of characters to read from an XML or JSON event when auto extracting
extraction_cutoff = 5000
extract_all = true

[stats]
maxresultrows = 50000
maxvalues = 0
maxvaluesize = 0
# for streamstats's maximum window size
max_stream_window = 10000
# for rdigest, used to approximate order statistics (median, percentiles)
rdigest_k = 100
rdigest_maxnodes = 1

[sistats]
maxvalues = 0
maxvaluesize = 0
rdigest_k = 100
rdigest_maxnodes = 1
max_valuemap_bytes = 100000

[top]
maxresultrows = 50000
# maximum distinct value vectors to keep track of
maxvalues = 0
maxvaluesize = 0


[transactions]
# maximum number of open transaction or events in open
# transaction before transaction eviction happens
maxopentxn    = 5000
maxopenevents = 100000


[typer]
# in eventtyping, pay attention to first N characters of any
# attribute (e.g., _raw), including individual tokens. Can be
# overridden by supplying the typer operator with the argument
# maxlen (e.g. "|typer maxlen=300").
maxlen = 10000


[authtokens]
# Time before an auth token will be expire if not used, in seconds.
# 0 will indicate no timeout.
expiration_time = 3600


[scheduler]
saved_searches_disabled = false

# the maximum number of searches the scheduler can run, as a percentage
# of the maximum number of concurrent searches
max_searches_perc  = 50

# fraction of concurrent scheduler searches to use for auto summarization
auto_summary_perc  = 50

# Every search should run as soon possible after its next scheduled time.
# However, each is penalized by its average runtime thus allowing
# shorter-running searches to run sooner and not potentially starve.
#
# However, since many searches run in fractions of a second and the
# priority type is integral and based on seconds, adding a raw runtime is
# too small to alter the result.  Therefore, we scale the runtime.
priority_runtime_factor = 10

# A potential issue with the priority_runtime_factor is that now longer-running
# searches may get starved.  To balance this out, make a search's priority
# lower (better) the more times it's been skipped.
#
# The adjustment should be normalized by the search's period, i.e., an
# infrequent search that has been skipped should get a lower (better) score
# than a frequent search that has been skipped the same number of times.
#
# Eventually, this adjustment will outweigh any worse priority due to a long
# runtime. The priority_skipped_factor controls how quickly this happens.
priority_skipped_factor = 1

# The maximum number of minutes to defer running continuous scheduled searches
# while waiting for the KV Store to come up in order to load historical data.
# This is used to prevent gaps in continuous scheduled searches when splunkd
# was down.
#
# Use [<int>]<unit> to specify a duration; a missing <int> defaults to 1.
# Relevant units are: s, sec, second, secs, seconds, m, min, minute, mins,
# minutes.
# For example: "60s" = 60 seconds, "5m" = 5 minutes.
search_history_load_timeout = 2m

# The number of runtimes kept for each search that are used to calculate the
# historical everage runtime during search prioritization.
search_history_max_runtimes = 10

# The maximum amount of time to run missed continuous scheduled searches for
# once Splunk comes back up in the event it was down.
#
# Use [<int>]<unit> to specify a duration; a missing <int> defaults to 1.
# Relevant units are: min, minute, mins, minutes, h, hr, hour, hrs, hours, d,
# day, days, w, week, weeks, mon, month, months.
# For example: "5m" = 5 minutes, "1h" = 1 hour.
#
# A value of 0 means no lookback.
max_continuous_scheduled_search_lookback = 24h

# The amount of time to "look back" when reporting introspection statistics.
# For example: what is the number of dispatched searches in the last 60 minutes?
#
# Use [<int>]<unit> to specify a duration; a missing <int> defaults to 1.
# Relevant units are: m, min, minute, mins, minutes, h, hr, hour, hrs, hours,
# d, day, days, w, week, weeks.
# For example: "5m" = 5 minutes, "1h" = 1 hour.
introspection_lookback = 1h

# maximum number of results to load when triggering an action
max_action_results = 50000

action_execution_threads  = 2

actions_queue_size        = 100

actions_queue_timeout     = 30

alerts_max_count          = 50000

alerts_max_history        = 7d

alerts_expire_period      = 120

persistance_period         = 30

# maximum number of lock files to keep around for each scheduled search
# effective only if search head pooling is enabled, the most recent files are kept
max_lock_files = 5

# the lock file reaper should clean lock files that are this old (in seconds)
max_lock_file_ttl = 86400

max_per_result_alerts = 500

scheduled_view_timeout = 60m

# Scheduler timeout for printing a throttled warning message
# if we're hitting scheduler concurrency limits
concurrency_message_throttle_time = 10m

# by default the scheduler should not run jobs on itself in search head pooling mode
# it should dispatch to pool members.
shp_dispatch_to_slave = true
# in 6.3 and beyond Search Head Clustering has implemented role quota
# enforcement. Set this to false to disable this feature
shc_role_quota_enforcement = true

# do the quota checks only on the captain and do not do quota checks on the member
# this assumes that the captain has done proper checks and proper LOAD BALANCING
shc_local_quota_check = false

[auto_summarizer]
cache_timeout      = 600
maintenance_period = 1800
return_actions_with_normalized_ids = fromcontext
normalized_summaries = true
detailed_dashboard = true
shc_accurate_access_counts = false

[thruput]
# throughput limiting at index time
maxKBps = 0

[show_source]
# maximum events retriveable by show source
max_count = 10000
max_timebefore = 1day
max_timeafter = 1day
distributed = true
# maximum events we will request in the distributed show source. Likely all of these will not be used
distributed_search_limit = 30000

[ldap]
# maximum number of users we will attempt to precache from LDAP after reloading auth
max_users_to_precache = 1000
# controls whether we allow login when we find multiple entries with the same value for the username attribute
allow_multiple_matching_users = true

[reversedns]
# max percent of time allowed for reverse dns lookups for incoming
# forwarder connections before WARN is logged in splunkd.log
# sanity check diagnostic for slow lookups
rdnsMaxDutyCycle = 10

[viewstates]
# is the viewstate reaper enabled?
enable_reaper = true
# how often does the reaper run?
reaper_freq = 86400
# how many viewstates does the reaper consider "acceptable"?
reaper_soft_warn_level = 1000
# reaper eligibility age
ttl = 86400

[geostats]
# at the lowest level of the tree, i.e. ZL=0 (when we are zoomed out to the world level ), what is the
# size of each gridcell in terms of latitude and longitude (degrees)? Valid values for
# zl_0_gridcell_latspan are from 0 to 180.0, and for zl_0_gridcell_longspan are from 0 to 360.0
# Rest of the zoom level gridcell sizes are auto-tuning, i.e. will reduce by a factor of 2
# at each additional level
zl_0_gridcell_latspan = 22.5
zl_0_gridcell_longspan = 45.0
# configures the filtering/search strategy for events on the map
# currently experimental
filterstrategy = 2
# how many levels of clustering will be done in geostats.
maxzoomlevel = 9


[tscollect]
# default value of 'squashcase' arg if not specified by the command
squashcase = false
# default value of 'keepresults' arg if not specified by the command
keepresults = false
# the max allowed size of tsidx files to create in megabytes. '0' implies no limit
optimize_max_size_mb = 256

[tstats]
# whether we apply role-based search filters when users run tstats on normal index data (never applied on data from tscollect or datamodel acceleration)
apply_search_filter = true
# default value of 'summariesonly' arg if not specified by the command
summariesonly = false
# default value of 'allow_old_summaries' arg if not specified by the command
allow_old_summaries = false
# by default we retrieve up to ten million events at once from a TSIDX file when answering queries
chunk_size = 10000000

[pdf]
# the max number of rows that the pdfgen rendering engine (not PDF Report Server app) will render for any individual table or event listing
max_rows_per_table = 1000
# the number of seconds after which the pdfgen render endpoint will timeout if it has not yet finished rendering the PDF output
render_endpoint_timeout = 3600

[kvstore]
# the max number of accelerations that can be assigned to a single collection
# Valid values range from 0 to 50
max_accelerations_per_collection = 10
# the max number of fields that can be part of an acceleration
# Valid values range from 0 to 30
max_fields_per_acceleration = 10
# the max number of rows that will be returned per query
max_rows_per_query = 50000
# the max number of queries that can be run as part of the same batch
max_queries_per_batch = 1000
# the max size of a query result in MB
max_size_per_result_mb = 50
# the max size of a batch save operation in MB
max_size_per_batch_save_mb = 50
# the max number of documents of a batch save operation
max_documents_per_batch_save = 1000
# the max size of a batched query result in MB
max_size_per_batch_result_mb = 100
# the max number of rows in memory before flushing them to CSV projection
max_rows_in_memory_per_dump = 200
# the max number of threads to use for outputlookup
max_threads_per_outputlookup = 1

[http_input]
# The max number of tokens reported by logging input metrics.
max_number_of_tokens = 10000
# The interval (in seconds) of logging input metrics report.
metrics_report_interval = 60
# The max request content length.
max_content_length = 1000000
# The max number of ACK channels.
max_number_of_ack_channel = 1000000
# The max number of acked requests pending query.
max_number_of_acked_requests_pending_query = 10000000
# The max number of acked requests pending query per ACK channel.
max_number_of_acked_requests_pending_query_per_ack_channel = 1000000


[slow_peer_disconnect]
# Settings for the heuristic that will detect and disconnect slow peers
# towards the end of a search that has returned a large volume of data

disabled = true
# is this feature enabled.
# Defaults to true

batch_search_activation_fraction = 0.9
# the fraction of peers that must have completed before we start disconnecting
# this is only applicable to batch search because the slow peers will not hold
# back the fast peers.
# Defaults to 0.9

packets_per_data_point = 500
# Rate statistics will be sampled once every packets_per_data_point packets.

sensitivity = 0.3
# Sensitivity of the heuristic to newer values. For larger values of sensitivity
# the heuristic will give more weight to newer statistic.

grace_period_before_disconnect = 0.10
# If the heuristic consistently claims that the peer is slow for atleast
# <grace_period_before_disconnect>*life_time_of_collector seconds then only
# will we disconnect the peer

threshold_data_volume = 100
# The volume of uncompressed data that must have accumulated in KB from
# a peer before we consider them in the heuristic.

threshold_connection_life_time = 5
# All peers will be given an initial grace period of at least these many
# seconds before we consider them in the heuristic.

bound_on_disconnect_threshold_as_fraction_of_mean = 0.2
# If network is too homogenous resulting in very low standard deviations this
# value may be tweaked to ensure that the thresholds we set are not too close to
# the mean. If threshold is an upper bound thresold >= mean*(1+bound_on_threshold)
# and if the threshold is a lower bound threshold <= mean*(1-bound_on_threshold)
# The actual threshold is computed during the search based on the mean and std.
# deviations of network statistics.

[system_checks]

orphan_searches = enabled
# see limits.conf.spec file for details
